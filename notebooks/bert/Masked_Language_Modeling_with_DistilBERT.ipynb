{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxfuhQicBOLPhvCx7wd3vl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimitarpg13/transformer_examples/blob/main/notebooks/bert/Masked_Language_Modeling_with_DistilBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masked Language Modeling\n",
        "\n",
        "Masked Language Modeling predicts which words fits best a blanked words in a given sentence. These models take sentences with blanked text as input and their output are the possible values of the text for that mask. These models can attend to tokens bidirectionally. This means the model has full access o the tokens on the left and the right. Masked Language modeling is used before fine-tuning the model for the specific task at hand. For example, if you need to use a model in a specific domain models like BERT will treat the domain-specific words as rare tokens. Then one can train the masked language model using the corpus of words for the specific domain and then fine-tune the model on a downstream task then we will end up with better performing model; that is model with higher inference accuracy given the amount of training time and word corpus. Wtth regard to classification metrics there is no single correct answer. We evaulate the distribution of the masked values. Common metrics are cross-entropy loss and perplexity.\n",
        "\n",
        "We can use any plain text dataset and tokenize the text to mask the data."
      ],
      "metadata": {
        "id": "2jRHgf7aeUvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will tune [DistillRoBERTa](https://huggingface.co/distilbert/distilroberta-base) on the [r/askscience](https://www.reddit.com/r/askscience/) subset of the [ELI5](https://facebookresearch.github.io/ELI5/) dataset.\n",
        "\n",
        "We will start by loading the first 5,000 examples with the [ELI5-Category](https://huggingface.co/datasets/rexarski/eli5_category) Dataset using the Datasets library. But first we take care of installing the necessary libraries:\n",
        "\n"
      ],
      "metadata": {
        "id": "0_ipW5pDoThp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate\n",
        "!pip install -U datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdLTjFuPeTxY",
        "outputId": "eeae662f-38b9-45ac-bab8-2ca650be5032"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "eli5 = load_dataset(\"eli5_category\", split=\"train[:5000]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLjzXFB9p29K",
        "outputId": "efd3f66c-7f8b-4c0a-b589-50a3ab9fd648"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eli5 = eli5.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "_Qyl-DoMqLyc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eli5[\"train\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKZo9JATrEL3",
        "outputId": "fe2fe061-2fad-4600-99a5-d837aa15f508"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'q_id': '5p3u5z',\n",
              " 'title': 'What is the difference between low level programming language and high level language?',\n",
              " 'selftext': 'What is the difference between low level programming language and high level language? I have no knowledge of coding/computer language at all so please keep that in mind. Also examples of both would be great. Thanks!',\n",
              " 'category': 'Technology',\n",
              " 'subreddit': 'explainlikeimfive',\n",
              " 'answers': {'a_id': ['dco5c3p', 'dcob0eu', 'dco5qd6'],\n",
              "  'text': ['It\\'s basically how close it is to the \"real\" operations the computer is doing. For example, take a list of names you need to alphabetically sort. Using plain English to make it simpler, in a high level language you basically just say: Computer, sort the list alphabetically and the rest is handled by the in built functions of the language that convert this to machine code. In a low level language you need to be more specific, e.g.: Take the first name and compare it to the second. If the first letter is lower then put name A first. If they are the same then take the second letter... etc. Basically a high level language comes bundled with lots of operations simplified and with many abstractions that mean you don\\'t need to be thinking (mostly) about what the computer is *actually* doing, you only think about results. Whereas a low level language needs to constantly think about how the machine is actually wired and work appropriately. To expand a little as to which is \"better\", it depends on your purpose and requirements. Generally high level languages are preferred because they are easier and less error prone, and therefore generally cheaper. Also these days performance is usually about the same because high level languages are pretty good these days at generating low level code for the computer to run. However sometimes it\\'s possible to write a more efficient algorithm in a low level language compared to what the high level language produces. Also some devices are very idiosyncratic in their requirements and a low level language allows you to tailor code more specifically. Additionally since low level languages are typically older, they can produce code for basically any device (although this point is increasingly moot as high level languages are nowadays usually able to be compiled for just about anything).',\n",
              "   \"Low level languages hide less from you regarding what the computer is actually doing. For example, object oriented programming is a thing. However, cpu knows no such thing as an object. It's something that you can use that doesn't actually correspond to what cpu is doing, and thus it's actually difficult to estimate, when you summon an object, to guess what exactly your cpu is doing. Low level languages on the other hand deal with things that cpu is actually doing. You'll deal with memory addresses, allocating memory, releasing memory, consider when to fetch data from ram to cpu registers and how that works out... it's not easy to write what you mean, but you know exactly what cpu is doing in response to these instructions\",\n",
              "   'Computers read simple instructions from the application code. This machine code is quite primitive, for example \"41\" might mean \"add the number in register a and register b and store the result in register c\", \"17\" might mean \"read the number stored in the memory at the address in register d and store it in register a\". To help remembering all these numbers we mostly use assembly language which is simple translations. It is simple to write an assembler that translates the assembly commands into machine code. Then we get to low level languages. Most notably C. This is a way to simplify the process of making assembly code by adding some structure to the language. So if you write \"int a = 2*b+c\" the compiler will break down the operations into simple assembly instructions. It makes it much easier to read and write which speeds up the programming. However you are still working a lot with memory addresses and the size of variables and such low level stuff. High level languages on the other hand is an extra step away from the machine code. They often abstract away a lot of the low level stuff like how much data you can store in a single variable and where in memory the data resides. This removes a huge workload from the programmer so it is possible to work faster.'],\n",
              "  'score': [6, 3, 3],\n",
              "  'text_urls': [[], [], []]},\n",
              " 'title_urls': ['url'],\n",
              " 'selftext_urls': ['url']}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing a dataset for masked language modeling\n",
        "\n",
        "Example:\n",
        "\n",
        "`[My] [name] [MASK] [Sylvain] [.]`\n",
        "               \n",
        "                  |\n",
        "                  V\n",
        "                 [is]\n",
        "\n",
        "`[I] [MASK] [at] [Hug] [##ging] [Face] [.]`\n",
        "\n",
        "          |\n",
        "          V\n",
        "        [work]\n",
        "\n",
        "We need to fill the masks\n",
        "\n",
        "```python\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "raw_datasets[\"train\"]\n",
        "\n",
        "Dataset({\n",
        "  features: ['text'],\n",
        "  num_rows: 36718\n",
        "})\n",
        "```\n",
        "\n",
        "Gather all of the text in one column in your dataset. Before we start the masking process we need to make all of the text the same length to batch them together. The first way to make the text the same length is the way we do that in text classification tasks - pad the short text sentences and truncate the long text sentences.\n",
        "\n",
        "Example:\n",
        "\n",
        "`[CLS] [My] [name] [is] [Sylvain] [.] [SEP]`\n",
        "\n",
        "`[CLS] [I] [MASK] [at] [Hug] [##ging] [SEP]`\n",
        "\n",
        "`[CLS] [Short] [text] [PAD] [PAD] [PAD] [SEP]`\n",
        "\n",
        "As we have seen when we repurpose data for text classification\n",
        "\n",
        "```python\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "raw_datasets = load_dataset(\"imdb\")\n",
        "raw_datasets = raw_datasets.remove_columns(\"label\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "context_length = 128\n",
        "\n",
        "def tokenize_pad_and_truncate(texts):\n",
        "  return tokenizer(texts[\"text\"], truncation=True, padding=\"max length\", max_length=context_length)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_pad_and_truncate, batched=True)\n",
        "\n",
        "```\n",
        "This padding and truncation is done automatically by `AutoTokenizer`.\n",
        "However, using `AutoTokenizer` we are going to loose a lot of text if the datasets are very long compared to the context length we have picked.\n",
        "\n",
        "![Figure: chunking on context length pieces](https://github.com/dimitarpg13/transformer_examples/blob/main/images/chunking_on_context_length_pieces.png?raw=1)\n",
        "\n",
        "We can chunk in pieces of length equal to the context length instead of discarding everything after the first chunk. We may end up with a remainder which we can keep in a end pad or ignore.\n",
        "\n",
        "We can implement this in practice with the following code which sets `return_overflowing_tokens` to `True` in the `tokenzier` call:\n",
        "\n",
        "```python\n",
        "def tokenize_and_chunk(texts):\n",
        "  return tokenizer(\n",
        "     texts[\"text\"], truncation=True, max_length=context_length,\n",
        "     return_overflowing_tokens=True\n",
        "  )\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "  tokenize_and_chunk, batched=True, remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "len(raw_datasets[\"train\"]), len(tokenized_datasets[\"train\"])\n",
        "\n",
        "      (36718, 47192)\n",
        "```\n",
        "\n",
        "This way of chunking is ideal if all of your text is very long. But this won't work nicely if there is a variety of lengths in the text. In this case the best option is to concatenate all of your text in one big string with a special token (depicted in orange) indicating when we pass from one document to another.\n",
        "\n",
        "![Figure: chunking on context length pieces](https://github.com/dimitarpg13/transformer_examples/blob/main/images/concatenate_in_one_big_string.png?raw=1)\n",
        "\n",
        "This is how this can be done in code:\n",
        "\n",
        "```python\n",
        "def tokenize_and_chunk(texts):\n",
        "  all_input_ids = []\n",
        "  for input_ids in tokenizer(texts[\"text\"])[\"input_ids\"]:\n",
        "    all_input_ids.extend(input_ids)\n",
        "    all_input_ids.append(tokenizer.eos_token_id)\n",
        "  \n",
        "  chunks = []\n",
        "  for idx in range(0, len(all_input_ids), context_length):\n",
        "    chunks.append(all_input_ids[idx: idx + context_length])\n",
        "  return {\"input_ids\": chunks}\n",
        "\n",
        "  tokenized_datasets = raw_datasets.map(tokenize_and_chunk, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "  len(raw_datasets[\"train\"]), len(tokenized_datasets[\"train\"])\n",
        "```\n",
        "\n",
        "The masking itself is done in a `DataCollator` instance:\n",
        "\n",
        "```python\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm_probability=0.15)\n",
        "\n",
        "```\n",
        "or\n",
        "```python\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "   tokenizer, mlm_probability=0.15, return_tensors=\"tf\"\n",
        ")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "lFruQmIrxXnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the next step is to load a DistilRoBERTa tokenizer to process the `text` subfield:"
      ],
      "metadata": {
        "id": "us-KXLabu78r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilroberta-base\")"
      ],
      "metadata": {
        "id": "wnbwr-uwypUE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice from the example above that the `text` field is actually nested inside `answers`. This means we need to extract the `text` subfield from its nested structure with the `flatten` method:"
      ],
      "metadata": {
        "id": "TaCUlgonDjfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eli5 = eli5.flatten()\n",
        "eli5[\"train\"][0]"
      ],
      "metadata": {
        "id": "B7KYtD8zDw-j",
        "outputId": "25413afc-8858-4321-b84c-d16705f4d8de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'q_id': '5p3u5z',\n",
              " 'title': 'What is the difference between low level programming language and high level language?',\n",
              " 'selftext': 'What is the difference between low level programming language and high level language? I have no knowledge of coding/computer language at all so please keep that in mind. Also examples of both would be great. Thanks!',\n",
              " 'category': 'Technology',\n",
              " 'subreddit': 'explainlikeimfive',\n",
              " 'answers.a_id': ['dco5c3p', 'dcob0eu', 'dco5qd6'],\n",
              " 'answers.text': ['It\\'s basically how close it is to the \"real\" operations the computer is doing. For example, take a list of names you need to alphabetically sort. Using plain English to make it simpler, in a high level language you basically just say: Computer, sort the list alphabetically and the rest is handled by the in built functions of the language that convert this to machine code. In a low level language you need to be more specific, e.g.: Take the first name and compare it to the second. If the first letter is lower then put name A first. If they are the same then take the second letter... etc. Basically a high level language comes bundled with lots of operations simplified and with many abstractions that mean you don\\'t need to be thinking (mostly) about what the computer is *actually* doing, you only think about results. Whereas a low level language needs to constantly think about how the machine is actually wired and work appropriately. To expand a little as to which is \"better\", it depends on your purpose and requirements. Generally high level languages are preferred because they are easier and less error prone, and therefore generally cheaper. Also these days performance is usually about the same because high level languages are pretty good these days at generating low level code for the computer to run. However sometimes it\\'s possible to write a more efficient algorithm in a low level language compared to what the high level language produces. Also some devices are very idiosyncratic in their requirements and a low level language allows you to tailor code more specifically. Additionally since low level languages are typically older, they can produce code for basically any device (although this point is increasingly moot as high level languages are nowadays usually able to be compiled for just about anything).',\n",
              "  \"Low level languages hide less from you regarding what the computer is actually doing. For example, object oriented programming is a thing. However, cpu knows no such thing as an object. It's something that you can use that doesn't actually correspond to what cpu is doing, and thus it's actually difficult to estimate, when you summon an object, to guess what exactly your cpu is doing. Low level languages on the other hand deal with things that cpu is actually doing. You'll deal with memory addresses, allocating memory, releasing memory, consider when to fetch data from ram to cpu registers and how that works out... it's not easy to write what you mean, but you know exactly what cpu is doing in response to these instructions\",\n",
              "  'Computers read simple instructions from the application code. This machine code is quite primitive, for example \"41\" might mean \"add the number in register a and register b and store the result in register c\", \"17\" might mean \"read the number stored in the memory at the address in register d and store it in register a\". To help remembering all these numbers we mostly use assembly language which is simple translations. It is simple to write an assembler that translates the assembly commands into machine code. Then we get to low level languages. Most notably C. This is a way to simplify the process of making assembly code by adding some structure to the language. So if you write \"int a = 2*b+c\" the compiler will break down the operations into simple assembly instructions. It makes it much easier to read and write which speeds up the programming. However you are still working a lot with memory addresses and the size of variables and such low level stuff. High level languages on the other hand is an extra step away from the machine code. They often abstract away a lot of the low level stuff like how much data you can store in a single variable and where in memory the data resides. This removes a huge workload from the programmer so it is possible to work faster.'],\n",
              " 'answers.score': [6, 3, 3],\n",
              " 'answers.text_urls': [[], [], []],\n",
              " 'title_urls': ['url'],\n",
              " 'selftext_urls': ['url']}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nQ110icTEBOi"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}