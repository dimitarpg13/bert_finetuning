{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeqtdOPQMXuMP+47p5qLdm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimitarpg13/transformer_examples/blob/main/notebooks/bert/Masked_Language_Modeling_with_DistilBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masked Language Modeling\n",
        "\n",
        "Masked Language Modeling predicts which words fits best a blanked words in a given sentence. These models take sentences with blanked text as input and their output are the possible values of the text for that mask. These models can attend to tokens bidirectionally. This means the model has full access o the tokens on the left and the right. Masked Language modeling is used before fine-tuning the model for the specific task at hand. For example, if you need to use a model in a specific domain models like BERT will treat the domain-specific words as rare tokens. Then one can train the masked language model using the corpus of words for the specific domain and then fine-tune the model on a downstream task then we will end up with better performing model; that is model with higher inference accuracy given the amount of training time and word corpus. Wtth regard to classification metrics there is no single correct answer. We evaulate the distribution of the masked values. Common metrics are cross-entropy loss and perplexity.\n",
        "\n",
        "We can use any plain text dataset and tokenize the text to mask the data."
      ],
      "metadata": {
        "id": "2jRHgf7aeUvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will tune [DistillRoBERTa](https://huggingface.co/distilbert/distilroberta-base) on the [r/askscience](https://www.reddit.com/r/askscience/) subset of the [ELI5](https://facebookresearch.github.io/ELI5/) dataset.\n",
        "\n",
        "We will start by loading the first 5,000 examples with the [ELI5-Category](https://huggingface.co/datasets/rexarski/eli5_category) Dataset using the Datasets library. But first we take care of installing the necessary libraries:\n",
        "\n"
      ],
      "metadata": {
        "id": "0_ipW5pDoThp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate\n",
        "!pip install -U datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdLTjFuPeTxY",
        "outputId": "bbe69f8d-4590-4582-c376-f15a40742837"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "eli5 = load_dataset(\"eli5_category\", split=\"train[:5000]\")"
      ],
      "metadata": {
        "id": "dLjzXFB9p29K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eli5 = eli5.train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "_Qyl-DoMqLyc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eli5[\"train\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKZo9JATrEL3",
        "outputId": "b6a17209-471c-4802-ecf2-ce3b28fb1ab6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'q_id': '5ox18t',\n",
              " 'title': 'If electricity travels at 300k meters per second, why does it take several hours to charge some lithium ion batteries.',\n",
              " 'selftext': '',\n",
              " 'category': 'Physics',\n",
              " 'subreddit': 'explainlikeimfive',\n",
              " 'answers': {'a_id': ['dcmrh7r',\n",
              "   'dcmvwsg',\n",
              "   'dcmq9w3',\n",
              "   'dcmo5v0',\n",
              "   'dcmqqwh',\n",
              "   'dcmtn4h'],\n",
              "  'text': [\"A battery isn't simply 'storing electricity' the same way a glass stores water. The electricity that flows to a battery is used to reverse a chemical reaction. The electricity is converted into chemical potential energy. This is the part that takes some time. Since it isn't a 100% efficient process, excess heat is generated. If you were to charge the battery super quickly, bad things would happen. When the battery is being used to power something, the chemical potential energy is being converted back into electrical energy.\",\n",
              "   \"Electrons in a conductor don't travel nearly that fast. The velocity that the electrons themselves travel at is referred to as the [drift velocity]( URL_1 ), which is actually very slow (typically less than a millimetre per second). What travels quickly is the electrical *signal* - when a voltage is applied, the electrons at the far end of the conductor start moving almost instantly. You can imagine a pipe filled with water: * The velocity of the water doesn't tell you how long it's going to take to fill your tank - you would also need to know how big your pipe is. What you want to know is how *much* water is flowing, not how fast it's flowing. * When you start pumping water, the disturbance will propogate at the speed of sound in water (which is about 1.5km/s), but that doesn't mean the water itself is travelling that fast. The water that starts flowing out the end of the pipe when you turn on the pumps is water that was already sitting near the end of the pipe. The reason you can't supply a lithium battery with a higher current to make it charge faster is because doing so can damage the battery. Lithium batteries rely on a chemical reaction to store energy, which can only proceeed so fast. If too much current is supplied, lithium metal gets deposited on the electrodes, which results in loss of capacity as it is the lithium ions that store the charge. If this results in a short circuit, it can lead to thermal runaway and the [catastrophic failure of the battery]( URL_0 ). Lithium batteries must be used within specified voltage and temperature ranges in order to remain safe to use.\",\n",
              "   'If you open the tap, water starts flowing out of it quickly. It still takes a while until your bathtub is full. 300 million meters per second, by the way.',\n",
              "   \"Those two things aren't related. Electricity travels that fast. That's why when you flick on your lights, it's basically instant. But a lithium ion battery takes several hours to charge because it holds *a lot* of electricity. The issue isn't how fast electricity is traveling, but how much has to pile in before a battery is charged.\",\n",
              "   'I bullet travels faster than the speed of sound, but you would still take hours to fill a house with bullets fired from your gun.',\n",
              "   '\"If light travels 300k meters per second, why does it take several days to get a tan?\"'],\n",
              "  'score': [16, 9, 7, 6, 5, 3],\n",
              "  'text_urls': [[],\n",
              "   ['https://www.youtube.com/watch?v=5J96ywv7yAM',\n",
              "    'https://en.wikipedia.org/wiki/Drift_velocity'],\n",
              "   [],\n",
              "   [],\n",
              "   [],\n",
              "   []]},\n",
              " 'title_urls': ['url'],\n",
              " 'selftext_urls': ['url']}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing a dataset for masked language modeling\n",
        "\n",
        "Example:\n",
        "\n",
        "`[My] [name] [MASK] [Sylvain] [.]`\n",
        "               \n",
        "                  |\n",
        "                  V\n",
        "                 [is]\n",
        "\n",
        "`[I] [MASK] [at] [Hug] [##ging] [Face] [.]`\n",
        "\n",
        "          |\n",
        "          V\n",
        "        [work]\n",
        "\n",
        "We need to fill the masks\n",
        "\n",
        "```python\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "raw_datasets[\"train\"]\n",
        "\n",
        "Dataset({\n",
        "  features: ['text'],\n",
        "  num_rows: 36718\n",
        "})\n",
        "```\n",
        "\n",
        "Gather all of the text in one column in your dataset. Before we start the masking process we need to make all of the text the same length to batch them together. The first way to make the text the same length is the way we do that in text classification tasks - pad the short text sentences and truncate the long text sentences.\n",
        "\n",
        "Example:\n",
        "\n",
        "`[CLS] [My] [name] [is] [Sylvain] [.] [SEP]`\n",
        "\n",
        "`[CLS] [I] [MASK] [at] [Hug] [##ging] [SEP]`\n",
        "\n",
        "`[CLS] [Short] [text] [PAD] [PAD] [PAD] [SEP]`\n",
        "\n",
        "As we have seen when we repurpose data for text classification\n",
        "\n",
        "```python\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "raw_datasets = load_dataset(\"imdb\")\n",
        "raw_datasets = raw_datasets.remove_columns(\"label\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "context_length = 128\n",
        "\n",
        "def tokenize_pad_and_truncate(texts):\n",
        "  return tokenizer(texts[\"text\"], truncation=True, padding=\"max length\", max_length=context_length)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_pad_and_truncate, batched=True)\n",
        "\n",
        "```\n",
        "This padding and truncation is done automatically by `AutoTokenizer`.\n",
        "However, using `AutoTokenizer` we are going to loose a lot of text if the datasets are very long compared to the context length we have picked.\n",
        "\n",
        "![Figure: chunking on context length pieces](https://github.com/dimitarpg13/transformer_examples/blob/main/images/chunking_on_context_length_pieces.png?raw=1)\n",
        "\n",
        "We can chunk in pieces of length equal to the context length instead of discarding everything after the first chunk. We may end up with a remainder which we can keep in a end pad or ignore.\n",
        "\n",
        "We can implement this in practice with the following code which sets `return_overflowing_tokens` to `True` in the `tokenzier` call:\n",
        "\n",
        "```python\n",
        "def tokenize_and_chunk(texts):\n",
        "  return tokenizer(\n",
        "     texts[\"text\"], truncation=True, max_length=context_length,\n",
        "     return_overflowing_tokens=True\n",
        "  )\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "  tokenize_and_chunk, batched=True, remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "len(raw_datasets[\"train\"]), len(tokenized_datasets[\"train\"])\n",
        "\n",
        "      (36718, 47192)\n",
        "```\n",
        "\n",
        "This way of chunking is ideal if all of your text is very long. But this won't work nicely if there is a variety of lengths in the text. In this case the best option is to concatenate all of your text in one big string with a special token (depicted in orange) indicating when we pass from one document to another.\n",
        "\n",
        "![Figure: chunking on context length pieces](https://github.com/dimitarpg13/transformer_examples/blob/main/images/concatenate_in_one_big_string.png?raw=1)\n",
        "\n",
        "This is how this can be done in code:\n",
        "\n",
        "```python\n",
        "def tokenize_and_chunk(texts):\n",
        "  all_input_ids = []\n",
        "  for input_ids in tokenizer(texts[\"text\"])[\"input_ids\"]:\n",
        "    all_input_ids.extend(input_ids)\n",
        "    all_input_ids.append(tokenizer.eos_token_id)\n",
        "  \n",
        "  chunks = []\n",
        "  for idx in range(0, len(all_input_ids), context_length):\n",
        "    chunks.append(all_input_ids[idx: idx + context_length])\n",
        "  return {\"input_ids\": chunks}\n",
        "\n",
        "  tokenized_datasets = raw_datasets.map(tokenize_and_chunk, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "  len(raw_datasets[\"train\"]), len(tokenized_datasets[\"train\"])\n",
        "```\n",
        "\n",
        "The masking itself is done in a `DataCollator` instance:\n",
        "\n",
        "```python\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm_probability=0.15)\n",
        "\n",
        "```\n",
        "or\n",
        "```python\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "   tokenizer, mlm_probability=0.15, return_tensors=\"tf\"\n",
        ")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "lFruQmIrxXnG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wnbwr-uwypUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jjw21XbHymxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KOpa9swNyYdJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}